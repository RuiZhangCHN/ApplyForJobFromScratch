## Summary Of Tasksä»»åŠ¡æ€»è§ˆ

### åºåˆ—åˆ†ç±»
åºåˆ—åˆ†ç±»æ—¨åœ¨æ ¹æ®ç»™å®šæ•°é‡çš„ç±»åˆ«å¯¹åºåˆ—è¿›è¡Œåˆ†ç±»ã€‚æ¯”å¦‚GLUE.

ä¸‹è¿°æ˜¯åœ¨sst-2è¿›è¡Œå¾®è°ƒçš„æ¨¡å‹ï¼Œè¯†åˆ«åºåˆ—æƒ…æ„Ÿç±»å‹æ˜¯æ­£è¿˜æ˜¯è´Ÿã€‚
```
from transformers import pipeline

nlp = pipeline("sentiment-analysis")

result = nlp("I hate you")[0]
print(f"label: {result['label']}, with score: {round(result['score'], 4)}")

===> label: NEGATIVE, with score: 0.9991

result = nlp("I love you")[0]
print(f"label: {result['label']}, with score: {round(result['score'], 4)}")

===> label: POSITIVE, with score: 0.9999
```

ä¸‹è¿°æ˜¯åˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯ä¸æ˜¯äº’ç›¸è§£é‡Šçš„æƒ…å½¢ã€‚å…¶è¿‡ç¨‹å¤§è‡´å¦‚ä¸‹ï¼š
- é¦–å…ˆä»checkpointä¸­å®ä¾‹åŒ–åˆ†è¯å™¨å’Œæ¨¡å‹ã€‚
- ä½¿ç”¨ç‰¹å®šäºæ¨¡å‹çš„åˆ†éš”ç¬¦ã€Tokenç±»å‹Idå’Œæ³¨æ„åŠ›Maskï¼Œå°†ä¸¤ä¸ªå¥å­æ„å»ºæˆä¸€ä¸ªåºåˆ—ã€‚
- å°†åºåˆ—ä¼ é€’ç»™æ¨¡å‹ï¼Œå°†å…¶åˆ†ç±»ä¸ºä¸¤ä¸ªç±»åˆ«ä¹‹ä¸€ï¼š0ï¼ˆä¸æ˜¯ï¼‰æˆ–1ï¼ˆæ˜¯ï¼‰
- è®¡ç®—ç»“æœçš„softmaxä»¥è·å¾—ç±»çš„æ¦‚ç‡
- æ‰“å°ç»“æœ
```
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased-finetuned-mrpc")
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased-finetuned-mrpc")

classes = ["not paraphrase", "is paraphrase"]

sequence_0 = "The company HuggingFace is based in New York City"
sequence_1 = "Apples are especially bad for your health"
sequence_2 = "HuggingFace's headquarters are situated in Manhattan"

paraphrase = tokenizer(sequence_0, sequence_2, return_tensors="pt")
not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors="pt")

paraphrase_classification_logits = model(**paraphrase).logits
not_paraphrase_classification_logits = model(**not_paraphrase).logits

paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]
not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]

# Should be paraphrase
for i in range(len(classes)):
     print(f"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%")

===> not paraphrase: 10%
===> is paraphrase: 90%

# Should not be paraphrase
for i in range(len(classes)):
    print(f"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%")
===> not paraphrase: 94%
===> is paraphrase: 6%
```

### æå–å¼QA
æå–å¼QAæ˜¯ä»ç»™å®šé—®é¢˜çš„æ–‡æœ¬ä¸­æå–ç­”æ¡ˆçš„ä»»åŠ¡ã€‚æ¯”å¦‚SQuADã€‚

(æ³¨ï¼šä¸‹é¢è¿™æ®µä»£ç æˆ‘è¿è¡Œä¸èµ·æ¥)
ä¸‹é¢æ˜¯ä½¿ç”¨pipelineè¿›è¡ŒQAçš„ç¤ºä¾‹ï¼Œå®ƒä½¿ç”¨äº†ä»SQuADä¸Šå¾®è°ƒçš„æ¨¡å‹ã€‚
```
from transformers import pipeline

nlp = pipeline("question-answering")

context = r"""
    Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a
    question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune
    a model on a SQuAD task, you may leverage the examples/question-answering/run_squad.py script.
    """
```
è¿”å›æå–çš„ç­”æ¡ˆï¼Œç½®ä¿¡åº¦ï¼Œåœ¨æ–‡ä¸­çš„å¼€å§‹å’Œç»“æŸä½ç½®
```
result = nlp(question="What is extractive question answering?", context=context)
print(f"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}")

===> Answer: 'the task of extracting an answer from a text given a question.', score: 0.6226, start: 34, end: 96

result = nlp(question="What is a good example of a question answering dataset?", context=context)
print(f"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}")

===> Answer: 'SQuAD dataset,', score: 0.5053, start: 147, end: 161
```
ä¸‹é¢æ˜¯ä½¿ç”¨æ¨¡å‹è¿›è¡ŒQAçš„è¿‡ç¨‹ï¼Œå¤§è‡´å¦‚ä¸‹ï¼š
- é¦–å…ˆä»checkpointä¸­å®ä¾‹åŒ–åˆ†è¯å™¨å’Œæ¨¡å‹ã€‚
- å®šä¹‰æ–‡æœ¬å’Œä¸€äº›é—®é¢˜
- éå†é—®é¢˜ï¼Œä½¿ç”¨ç‰¹å®šäºæ¨¡å‹çš„åˆ†éš”ç¬¦ã€Tokenç±»å‹Idå’Œæ³¨æ„åŠ›Maskï¼Œå°†ä¸¤ä¸ªå¥å­æ„å»ºæˆä¸€ä¸ªåºåˆ—ã€‚
- å°†åºåˆ—ä¼ é€’ç»™æ¨¡å‹ï¼Œæ¨¡å‹çš„è¾“å‡ºæ˜¯æ•´ä¸ªå›ç­”çš„èµ·ç‚¹å’Œç»ˆç‚¹èŒƒå›´
- è®¡ç®—ç»“æœçš„softmaxä»¥è·å¾—åœ¨tokenä¸Šçš„æ¦‚ç‡
- ä»èµ·å§‹å’Œç»ˆç‚¹è·å–tokenï¼Œå°†ä»–ä»¬è½¬åŒ–ä¸ºå­—ç¬¦ä¸²
- æ‰“å°ç»“æœ
```
from transformers import AutoTokenizer, AutoModelForQuestionAnswering
import torch

tokenizer = AutoTokenizer.from_pretrained("bert-large-uncased-whole-word-masking-finetuned-squad")
model = AutoModelForQuestionAnswering.from_pretrained("bert-large-uncased-whole-word-masking-finetuned-squad")

text = r"""
    ğŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose
    architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural
    Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between
    TensorFlow 2.0 and PyTorch.
"""

questions = [
    "How many pretrained models are available in ğŸ¤— Transformers?",
    "What does ğŸ¤— Transformers provide?",
    "ğŸ¤— Transformers provides interoperability between which frameworks?",
]

for question in questions:
    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors="pt")
    input_ids = inputs["input_ids"].tolist()[0]

    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)
    outputs = model(**inputs)
    answer_start_scores = outputs.start_logits
    answer_end_scores = outputs.end_logits

    answer_start = torch.argmax(
        answer_start_scores
    )  # Get the most likely beginning of answer with the argmax of the score
    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score

    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))

    print(f"Question: {question}")
    print(f"Answer: {answer}")

===> Question: How many pretrained models are available in ğŸ¤— Transformers?
===> Answer: over 32 +
===> Question: What does ğŸ¤— Transformers provide?
===> Answer: general - purpose architectures
===> Question: ğŸ¤— Transformers provides interoperability between which frameworks?
===> Answer: tensorflow 2 . 0 and pytorch
```
æ³¨ï¼šä¸Šé¢çš„ä»£ç åœ¨è·å–answer_start_scoreså’Œanswer_end_scoresçš„æ—¶å€™ä¼šæŠ¥é”™ï¼Œå› ä¸ºè¯´outputsæ˜¯ä¸€ä¸ªtupleã€‚æˆ‘è‡ªå·±æ”¹æˆäº†outputs[0]å’Œoutputs[1]å°±å¯ä»¥è¿è¡Œäº†ã€‚