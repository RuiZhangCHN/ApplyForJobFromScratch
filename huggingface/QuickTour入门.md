## QuickTourå…¥é—¨
### 1. ä½¿ç”¨pipelineæ„å»ºä¸€ä¸ªæƒ…æ„Ÿåˆ†ç±»å™¨
```
from transformers import pipeline

classifier = pipeline('sentiment-analysis')

# å¯¹ä¸€ä¸ªå¥å­è¿›è¡Œåˆ†ç±»
classifier('We are very happy to show you the ğŸ¤— Transformers library.')
# å¯¹ä¸€å †å¥å­è¿›è¡Œåˆ†ç±»
results = classifier(["We are very happy to show you the ğŸ¤— Transformers library.",
                      "We hope you don't hate it."])
```

### 2. ä½¿ç”¨åˆ«çš„æ¨¡å‹è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»
é¦–å…ˆéœ€è¦é€‰æ‹©ä¸€ä¸ªé¢„è®­ç»ƒçš„æ¨¡å‹ç±»å‹ï¼Œä»é¢„è®­ç»ƒçš„æ¨¡å‹é…ç½®ä¸­å–å‡ºåˆ†è¯å™¨å’Œæ¨¡å‹ã€‚æœ€åä½¿ç”¨pipelineæ„å»ºæ¨¡å‹ã€‚
```
from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)
```

### 3. ä¸ä½¿ç”¨pipelineè¿›è¡Œåˆ†ç±»
é¦–å…ˆç±»ä¼¼å‰é¢çš„æ“ä½œï¼Œå–å‡ºæ¨¡å‹å’Œåˆ†è¯å™¨
```
from transformers import AutoTokenizer, AutoModelForSequenceClassification
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
```
å¯¹è¾“å…¥è¿›è¡Œåˆ†è¯
```
inputs = tokenizer("We are very happy to show you the ğŸ¤— Transformers library.")
```
ç±»ä¼¼äºä¸Šä¸€è¡Œï¼Œå¦‚æœæ˜¯å¯¹ä¸€å¤§å †æ•°æ®è¿›è¡Œåˆ†è¯ï¼Œå¯ä»¥å¾—åˆ°ä¸€ä¸ªæ•°æ®batchã€‚è¿™é‡Œå¯ä»¥å¤„ç†paddingå’Œæˆªæ–­ï¼Œæ³¨æ„è¿”å›çš„æ—¶å€™æŒ‡å®šæ¨¡å‹ç±»å‹ptè¿˜æ˜¯tfã€‚
```
pt_batch = tokenizer(
    ["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    return_tensors="pt")
```
ç„¶åå°†ä¸Šé¢çš„batchä¼ å…¥åˆ°æ¨¡å‹ä¸­ã€‚æ³¨æ„å¯¹äºpytorchçš„ç‰ˆæœ¬éœ€è¦ä½¿ç”¨ä¸¤ä¸ªæ˜Ÿå·\*\*æ¥unpackä¸Šä¸€æ­¥å¾—åˆ°çš„è¾“å…¥ã€‚
```
pt_outputs = pt_model(**pt_batch)
```
ç„¶åè¿˜å¯ä»¥åœ¨å‰é¢çš„è¾“å‡ºç»“æœä¸ŠåŠ è¿›å»æ¿€æ´»å‡½æ•°
```
import torch.nn.functional as F
pt_predictions = F.softmax(pt_outputs[0], dim=-1)
```
å¦‚æœåœ¨å‰é¢è°ƒç”¨æ¨¡å‹çš„æ—¶å€™ç»™å‡ºç›®æ ‡labelçš„å€¼ï¼Œè¿˜å¯ä»¥å¾—åˆ°æŸå¤±å’Œæœ€åçš„pt_outputsç»“æœï¼š
```
import torch
pt_outputs = pt_model(**pt_batch, labels = torch.tensor([1, 0]))
```
### 4. æ¨¡å‹çš„ä¿å­˜
è®°å¾—ä¿å­˜åˆ†è¯å™¨å’Œæ¨¡å‹
```
tokenizer.save_pretrained(save_directory)
model.save_pretrained(save_directory)
```
### 5. æ¨¡å‹çš„åŠ è½½
åŠ è½½çš„æ—¶å€™å¦‚æœæ˜¯ä»å¦ä¸€ä¸ªåç«¯è®­ç»ƒå¾—åˆ°çš„æ¨¡å‹ï¼ŒåŠ è½½Modelæ—¶è¦è¿›è¡ŒæŒ‡æ˜
```
tokenizer = AutoTokenizer.from_pretrained(save_directory)
model = TFAutoModel.from_pretrained(save_directory, from_pt=True)
```
æˆ–
```
tokenizer = AutoTokenizer.from_pretrained(save_directory)
model = AutoModel.from_pretrained(save_directory, from_tf=True)
```
### 6. è¾“å‡ºæ‰€æœ‰éšè—å±‚å’Œæ³¨æ„åŠ›
å¯ä»¥è®©æ¨¡å‹è¾“å‡ºæ‰€æœ‰éšè—å±‚å’Œæ³¨æ„åŠ›æƒé‡
```
pt_outputs = pt_model(**pt_batch, output_hidden_states=True, output_attentions=True)
all_hidden_states, all_attentions = pt_outputs[-2:]
```

### 7. å®šåˆ¶æ¨¡å‹
å¯ä»¥é€šè¿‡Configæ–‡ä»¶æ¥ä¿®æ”¹æ¨¡å‹çš„ä¸€äº›å‚æ•°ã€‚æˆ‘ä¸ªäººçš„ç†è§£è¿™é‡Œåº”è¯¥æ˜¯ç”±ä¸¤ç§Configï¼Œä¸€ç§æ˜¯åŸºç±»Configï¼Œé€šå¸¸è€Œè¨€ä¿®æ”¹åŸºç±»Configçš„å†…å®¹ä¸ä¼šå½±å“åˆ°æ¨¡å‹çš„ç»“æ„ï¼Œå› æ­¤ä¿®æ”¹åŸºç±»Configçš„æ—¶å€™å¯ä»¥ä½¿ç”¨é€šç”¨çš„pretrainedå¾—åˆ°çš„æ¨¡å‹ï¼›
ä½†æ˜¯å¯¹äºéåŸºç±»çš„Configï¼Œä¾‹å¦‚ç‰¹å®šçš„DistilBertConfigï¼Œä¿®æ”¹è¿™ç±»Configä¸­çš„å‚æ•°ä¼šæ¶‰åŠæ¨¡å‹ç»“æ„çš„æ”¹å˜ï¼Œå°±éœ€è¦ä»é›¶å¼€å§‹è®­ç»ƒä¸€ä¸ªæ¨¡å‹ã€‚

ä»é›¶å¼€å§‹æ„å»ºæ¨¡å‹çš„ä»£ç 
```
from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification
config = DistilBertConfig(n_heads=8, dim=512, hidden_dim=4*512)
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertForSequenceClassification(config)
```

åªä¿®æ”¹BaseConfigå› æ­¤è¿˜å¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„ä»£ç 
```
from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification
model_name = "distilbert-base-uncased"
model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=10)
tokenizer = DistilBertTokenizer.from_pretrained(model_name)
```