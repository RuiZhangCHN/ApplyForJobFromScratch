# 自然语言处理NLP专题

### 选择题
#### 数据平滑
> 在统计语言模型中，通常以概率的形式描述任意语句的可能性，利用最大相似度估计进行度量，对于一些低频词，无论如何扩大训练数据，出现的频度仍然很低，下列哪种方法可以解决这一问题（C） 
>
> A. 一元切分 B. 一元文法 C. 数据平滑 D. N元文法

为了解决未出现的新序列概率估计值为0的情况，提出了一些数据[平滑算法](https://blog.csdn.net/quicmous/article/details/52160940)，包括：
- Laplace法则：将计数器的初始值设为1而不是0，即r* = r+1
- Good-Turing：计算为r\* = (r+1) \* n_{r+1} / n_{r}，这样可以保证sum_r\* = sum_r
- Witten-Bell算法:如果测试过程中一个实例在训练语料库中未出现过，那么他就是一个新事物，也就是说，他是第一次出现。那么可以用在语料库中看到新实例（即第一次出现的实例）的概率来代替未出现实例的概率。即p=(不同的词的数量/词的总量)
- 线性插值算法：对于n-gram的情况，采用低阶gram的概率进行插值。例如p(w2|w1) = a\*p(w2|w1) + (1-a)\*p(w2)

#### NLP语料和数据
> 《同义词词林》的词类分类体系中，将词分为大类、种类、小类，下列说法正确的是（D）
> 
> A. 大类以小写字母表示 B. 小类以大写字母表示 C.中类以阿拉伯数字表示 D.中类有94个

大类（12）大写字母，中类（94）小写字母，小类（1438）数字，标题词（3933）。死记硬背个数没意义，嗯。

#### 传统方法
> \[多选\]下面哪些算法模型可以用来完成命名实体的任务（CDEF）
> 
> A. GBDT B. LDA C.HMM D.CRF E. LSTM F. Seq2Seq

包括：
- 基于规则的方法。根据语言学上预定义的规则。但是由于语言结构本身的不确定性，规则的制定上难度较大。
- 基于统计学的方法。利用统计学找出文本中存在的规律。主要有隐马尔可夫(HMM)、条件随机场(CRF)模型和Viterbi算法、支持向量机（Support Vector Machine, SVM）。
- 神经网络。LSTM+CRF模型，基于RNN的seq2seq模型等。

> \[多选\]命名实体识别是指出文本中的人名、地名等专有名词和时间等，其中有有监督的命名实体识别和无监督的命名实体识别，下列选项哪些是属于有监督的学习方法（BCD）
> 
> A. 字典法 B.决策树 C. 隐马尔可夫模型 D. 支持向量机

无它，这道题提醒我去复习一下BCD的基础知识。

> \[多选\]决策树有哪些常用的启发函数（ABC）
> 
> A. 最大信息增益 B. 最大信息增益率 C. 最大基尼系数 D. 最大交叉熵

待整理。[决策树的启发函数](https://www.jianshu.com/p/6a93d1455357)

> 下面说法错误的是(B)
> 
> A. 遗传算法直接以适应度作为搜索信息,无需导数等其他辅助信息
> B. 决策树算法对离散属性和连续属性进行建模
> C. Hapfield网络不仅有不动点吸引子,也有其它类型的吸引子
> D. 决策树是一种混合算法,它综合了多种不同的创建树的方法

待整理。

> 考虑两个分类器：1）核函数取二次多项式的SVM分类器和2）没有约束的高斯混合模型（每个类别为一个高斯模型）。我们对R2空间的点进行两类分类。假设数据完全可分，SVM分类器中不加松弛惩罚项，并且假设有足够多的训练数据来训练高斯模型的协方差。下面说法正确的是？(B)
> 
> A. SVM的VC维大于高斯混合模型的VC维
> B. SVM的VC维小于高斯混合模型的VC维
> C. 两个分类器的结构风险值相同
> D. 这两个分类器的VC维相同

待整理。

#### 深度网络
> \[多选\]下面哪些方法有助于解决深度网络的梯度消失问题（ACDEFG）
> 
> A. 控制网络深度 B. 使用Sigmoid C. 预训练+微调 D. 使用ReLU
> E. 使用Batch Normalization F. 使用残差 G. 使用LSTM

除了我知道的那几个，另外，使用预训练+微调、使用BN、使用LSTM都能解决。